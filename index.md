# Statistical and Machine Learning

## Course Overview
This course provides a bridge between classical statistical inference and modern machine learning methods. Machine learning is a set of techniques that allow computers to learn from data and experience rather than requiring humans to specify the desired behavior by hand. ML has become increasingly central both in AI as an academic field and in industry. This course provides a broad introduction to some of the most commonly used ML algorithms. It also introduces vital algorithmic principles that will serve as a foundation for more advanced courses, such as Probabilistic Learning and Reasoning, and Deep Learning.

We start with the foundations of statistical learning and non-parametric models. We then turn to parametric models: linear regression, logistic regression, softmax regression, and neural networks. We then move on to unsupervised learning, focusing in particular on probabilistic models, Principal Component Analysis (PCA), and K-means. Finally, we cover the basics of reinforcement learning and causal inference.

This course delves deeply into the mathematical foundations of machine learning and focuses on implementing, understanding, and debugging software programs for building machine learning models from data. This class makes heavy use of the mathematical and statistical prerequisites. Students are expected to be comfortable with technical material from multivariate probability theory, multivariate calculus, and linear algebra, and to apply these concepts to the description, design, and implementation of learning algorithms.

### Weekly Agenda at a Glance

| Week | Lecture Topic | Tutorial (Python/Colab) |
|:---|:---|:---|
| 1 | Foundations of Statistical Learning: Bias-Variance Tradeoff | Intro to Numpy & Visualizing Bias-Variance |
| 2 | Resampling Methods & Model Assessment | Implementing Cross-Validation & Bootstrap from Scratch |
| 3 | Linear Models & Regularization (Lasso, Ridge) | Ridge vs. Lasso: Predicting Economic Indicators |
| 4 | Non-Linear Models I: Random Forests & XGBoost | Predicting Loan Defaults with Gradient Boosting |
| 5 | Non-Linear Models II: Kernel Methods & SVM | Visualizing Hyperplanes & Kernel Tricks |
| 6 | Non-Linear Models III: Optimization Basics | Coding Gradient Descent Algorithms from Scratch |
| 7 | Neural Networks I: Perceptrons & SGD | Building a Simple Neural Network in PyTorch |
| 8 | Neural Networks II: Architectures & Backpropagation | Image Classification on Fashion-MNIST |
| 9 | Probabilistic Models: MLE & MAP Estimation | Implementing Naive Bayes for Text Classification |
| 10 | Multivariate Gaussians & Gaussian Discriminant Analysis (GDA) | GDA vs. Logistic Regression on Tabular Data |
| 11 | Unsupervised Learning: PCA, K-Means, & EM Algorithm | Customer Segmentation & Dimensionality Reduction |
| **12** | **Reinforcement Learning I: MDPs & Bellman Equations** | **Solving the Grid World Problem (Dynamic Programming)** |
| 13 | Reinforcement Learning II: Q-Learning | Implementing a Q-Learning Agent |
| 14 | Causal ML & Ethics | Double Machine Learning & SHAP Values |


### Materials and Resources

*   **Bishop, C. M.:** *Pattern Recognition and Machine Learning.*
*   **Bishop, C. M.:** *Deep Learning: Foundations and Concepts.*
*   **Hastie, T., Tibshirani, R., and Friedman, J.:** *The Elements of Statistical Learning (ESL).*
*   **MacKay, D. J. C.:** *Information Theory, Inference, and Learning Algorithms.*
*   **Barber, D.:** *Bayesian Reasoning and Machine Learning.*
*   **Sutton, R. S. and Barto, A. G.:** *Reinforcement Learning: An Introduction.*
*   **Deisenroth, M. P., Faisal, A. A., and Ong, C. S.:** *Mathematics for Machine Learning.*
*   **Shalev-Shwartz, S. and Ben-David, S.:** *Understanding Machine Learning: From Theory to Algorithms.*
*   **Murphy, K. P.:** *Machine Learning: A Probabilistic Perspective.*
